{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Preparation for the Analysis of Tourism Visits in the United States from Mexico and the Northern Triangle (El Salvador, Guatemala, Honduras)\n",
    "### Data Engineering Capstone Project\n",
    "This project and this document are part of the final project for the Data Engineering Nanodegree from Udacity (a.k.a. \"The Capstone Project\"). The goal is to use the skills and methods learned within the program, to run a Data Engineering project from scratch. We will use the project provided by Udacity, in which a large dataset of arrivals into the United States from all over the world, forms the basis of the data.\n",
    "\n",
    "#### Project Summary\n",
    "In this project, we start with a clear **data-driven goal**, from which we derive a **series of steps** to reach that goal using **large quantities of data**. This intends to replicate the typical process a data engineer goes through on a regular basis.\n",
    "\n",
    "- **An important note when reviewing the Jupyter notebook: in some cells you find the comment `#nfp` or something similar. This means that this specific cell is only intendes for showing and control purposes when developing, but not for the final production pipeline/script. In other words, if you skip running this cells, the end result of the script should still be the same**\n",
    "    - Optionally, we have provided documentation (`README.md`) and python etl-script (`Capstone_Project_script.py`) separately, both just separate parts from this notebook\n",
    "\n",
    "The goal:\n",
    "- Structure and prepare immigration and temperature data to allow the analysis of tourism activities to the U.S. from Mexico and the Northern Triangle (El Salvador, Guatemala, Mexico).\n",
    "    - One important constraint or requieremoent: we shall provide a fine granularity of data (low aggregation levels) to maximize the flexibility of the analysis to be done on our end data (a.k.a. analytics tables), e.g. pattern recognition, dashboards, etc.\n",
    "\n",
    "The data:\n",
    "1. Immigration Data from the United States I-94 Arrivals Programm\n",
    "2.  Global Surface Temperature Data recorded since the 18th century \n",
    "\n",
    "The series of steps for this project:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "    - Explain the project scope in more detail, as well as the data sources we are using\n",
    "    - State how the end solution should look like, the approach and the tools we are using\n",
    "* Step 2: Explore and Assess the Data\n",
    "    - Explore the datasets and identify data issues\n",
    "    - If necessary, modify, pre-process and clean the data\n",
    "    - Run checks to understand the data and test some assumptions\n",
    "* Step 3: Define the Data Model\n",
    "    - Present the conceptual data model\n",
    "    - List the steps from the data pipeline to build the data model\n",
    "* Step 4: Run ELT to Model the Data & Data Quality Checks\n",
    "    - Run the pipeline\n",
    "    - Run Data Quality checks\n",
    "    - Provide a Data Dictionary\n",
    "* Step 5: Document the Project\n",
    "    - Explain our choice of tools and technologies\n",
    "    - Offer some critical points of our own\n",
    "    - Talk about what possibilites arise in case of new scenarios or changed requirements for our analytics tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all module imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, DecimalType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create and configure Spark Session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# UDF Functions\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import types as T\n",
    "# Reference: https://knowledge.udacity.com/questions/66798\n",
    "def to_datetime_sas(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "udf_to_datetime_sas = udf(lambda x: to_datetime_sas(x), T.DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "##### Explain what you plan to do in the project in more detail:\n",
    "- Latin American countries have seen large migrations of their populations throughout the last decades ([National Library of Medicine](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4638184/)). The immigration stemming from Mexico and the so called Northern Triangle of Central America (El Salvador, Honduras, Guatemala) are specially relevant for the US and these countries. As of 2020 there are around 36 Mio. mexicans living in the United States, 2.2 Mio. Salvadorans, 1.5 Mio. Guatemalans and around 1 Mio. Hondurans ([US Census Bureau - 2020](https://data.census.gov/cedsci/table?q=B03001%3A%20HISPANIC%20OR%20LATINO%20ORIGIN%20BY%20SPECIFIC%20ORIGIN&g=&lastDisplayedRow=30&table=B03001&tid=ACSDT1Y2019.B03001&hidePreview=true)). Taking into account each countries home population, respectively: 130 Mio., 6.5 Mio., 16.9 Mio. and 9.9 Mio. ([World Bank](https://databank.worldbank.org/data/download/POP.pdf)) it becomes clear how economically relevant the population living in the US can be for these four countries. Although there are many economic factors at play involved with migration (in both receiving and giving countries), we will focus our scope in tourism, specifically the opportunity cost of tourism. Many latin americans travel to the US to visit family and friends, thus doing tourism abroad and not at home.\n",
    "- In this project we will focus on the **tourist** travelers of these **four latin american countries** to the United States in the **year 2016.** We will prepare data from different sources into analytics tables, such that interested parties can create analysis and potentially dashboards off of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### What is your end solution look like? What tools did you use?\n",
    "- Since we are working with large amounts of data, we are going to use to use a 'Data Lake' approach to 1. read the raw data, 2. process and transform it and 3. write it to data storage. From there, analytics can easily be run on top of our result (analytics) tables.\n",
    "- Technically, we are going to use PySpark, which leverages the Spark technology to do all three steps described above - even and specially for large amounts of data.\n",
    "- The proposed solution needs storage for the Input Data and for the Output Data. We do not give a fix technology, since any storage service that can cope with the amount of data handled here (in terms of storage and read/write capabilities), and we the formats (csv, sas, parquet), will do. An example storage service would be AWS S3.\n",
    "- On a high level, or ELT-approach can be represented as follows:\n",
    "    - - ![etl-diagramm](pics/data-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data \n",
    "##### Describe the data sets you're using. What data do you use? What type of information is included? \n",
    "- I94 Immigration Data:\n",
    "    - Raw data format: SAS-files\n",
    "    - This dataset contains arrivals data to the United States from the I-94 Arrivals Program. It provides factual data about the  type, length and mode of the arrival/visit as well as some few non-personal information from non US-visitors. Specifically, a row in the dataset represents an arrival from one visitor with stays of 1-night or more and visiting for different visa types (tourism, business or student). The data is for the calendar year of 2016, which in its complete state contains a couple of million rows, since the data is not aggregated (to our knowledge).\n",
    "    - Since we are, among other countries, interested in visits from Mexico it is worth noticing that regular Mexican visitors to the U.S. \"within the 25-mile (40 kilometer) frontier zone are not included\" ([Reference](https://www.trade.gov/i-94-arrivals-program)).\n",
    "    - For more information, you can check the data dictionary in the file 'I94_SAS_Labels_Descriptions.SAS' or check the source:\n",
    "        - The dataset was provided by the U.S. National Travel and Tourism Office > [link to source](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "    - Relevant data for this project:\n",
    "        - To reduce the data load on Spark processin, we filter directly after import for visits where citizens are from Mexico, and the so called 'North Triangle' (El Salvador, Guatemala, Honduras). We do not filter for residence, since for our purpose the country of residence is not relevant. To achieve the filtering for the values `582.0 (MEX), 528.0 (SAL), 576.0 (GUA), 577.0 (HON)` using the column `i94cit`.\n",
    "        - A note on importing: we noted that the data for June contained six more columns (34 columns) then all the other files (28 columns). So we delete the superfluous columns before appending the data from all months into a single dataframe.\n",
    "    \n",
    "- Temperature Data\n",
    "    - Raw data format: CSV\n",
    "    - This data contains global information about the earth surface temperature data. It can be sliced and viewed in different ways, e.g. by  country, state, major cities or cities in general. For some places, the measurement date as back as 1750 all the way until september 2013. The original dataset is massive with a combined 1.6 billion temperature records. However, the data we use for this analysis is already aggregated by city.\n",
    "    - The data contains two temperature-relevant columns: the first is the average temperature and the second is the uncertainty of the average temperature.\n",
    "    - Besides that it contains latitude and longitude information \n",
    "    - A note on the average temperature: it was first during the cleaning of the data that it became apparent, that the data was a monthly average and not a daily average for the cities, which was corroborated by the source (see second link):\n",
    "        - Source: Berkeley Earth through Kaggle > [link1 to source](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data), [link2 to source](http://berkeleyearth.org/data/)\n",
    "\n",
    "- Port Codes\n",
    "    - This is not a new dataset per se, but a subset of the data dictionary from the immigration data. As we will see later, to fulfill our data model, we need to be able to match the temperature data with the immigration data. We do this by matching with the dates (on a monthly basis) and with the cities.\n",
    "    - However, the immigration data contains only a column called `i94port`, with a three-letter code. The human-readable version of the port/city name can be found within the file 'I94_SAS_Labels_Descriptions.SAS', for the `i94port` field. We have extracted this information and prepared it in a tabular way, for us to use as a conversion table and make the matching between temperature and immigration data easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns in i94_jun16_sub.sas7bdat are different then the ones until one. Deleting unnecesary columns...\n",
      "Columns now match. Appending...\n"
     ]
    }
   ],
   "source": [
    "# Read immigration data\n",
    "dir_path = '../../data/18-83510-I94-Data-2016/'\n",
    "iteration = 0\n",
    "relevant_country_codes = [582.0, 528.0, 576.0, 577.0]\n",
    "\n",
    "for file in os.listdir(dir_path):\n",
    "    if iteration == 0:\n",
    "        df_immigration_month = spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "        .load(os.path.join(dir_path, file))\\\n",
    "        .filter(col('i94cit').isin(relevant_country_codes))\n",
    "    else:\n",
    "        df_temp = spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "        .load(os.path.join(dir_path, file))\\\n",
    "        .filter(col('i94cit').isin(relevant_country_codes))\n",
    "        \n",
    "        if df_immigration_month.columns != df_temp.columns:\n",
    "            print(f\"The columns in {file} are different then the ones until one. Deleting unnecesary columns...\")\n",
    "            df_temp = df_temp.drop('validres', 'delete_days', 'delete_mexl', 'delete_dup', 'delete_visa', 'delete_recdup')\n",
    "            \n",
    "            if df_immigration_month.columns != df_temp.columns:\n",
    "                print(\"Columns are still not matching\")\n",
    "            else:\n",
    "                print(\"Columns now match. Appending...\")\n",
    "        else:    \n",
    "            df_immigration_month = df_immigration_month.union(df_temp)\n",
    "\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95441.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20554.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>IB</td>\n",
       "      <td>9.250658e+10</td>\n",
       "      <td>06253</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95442.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20546.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.248180e+10</td>\n",
       "      <td>00069</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95444.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>KAN</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20585.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>NK</td>\n",
       "      <td>9.242971e+10</td>\n",
       "      <td>00826</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95445.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>CIN</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20627.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>F</td>\n",
       "      <td>None</td>\n",
       "      <td>AM</td>\n",
       "      <td>9.242279e+10</td>\n",
       "      <td>00630</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95448.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20625.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>KX</td>\n",
       "      <td>9.250986e+10</td>\n",
       "      <td>00106</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0  95441.0  2016.0     4.0   528.0   129.0     CHI  20545.0      1.0      NY   \n",
       "1  95442.0  2016.0     4.0   528.0   129.0     CHI  20545.0      1.0      FL   \n",
       "2  95444.0  2016.0     4.0   528.0   528.0     KAN  20545.0      1.0      FL   \n",
       "3  95445.0  2016.0     4.0   528.0   528.0     CIN  20545.0      1.0      CA   \n",
       "4  95448.0  2016.0     4.0   528.0   528.0     CHI  20545.0      1.0      FL   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0  20554.0   ...        None        M   1986.0  09302016      F   None   \n",
       "1  20546.0   ...        None        M   1986.0  09302016      M   None   \n",
       "2  20585.0   ...        None        M   1964.0  09302016      F   None   \n",
       "3  20627.0   ...        None        M   1958.0  09302016      F   None   \n",
       "4  20625.0   ...        None        M   1954.0  09302016      M   None   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0      IB  9.250658e+10  06253       B2  \n",
       "1      AA  9.248180e+10  00069       B2  \n",
       "2      NK  9.242971e+10  00826       B2  \n",
       "3      AM  9.242279e+10  00630       B2  \n",
       "4      KX  9.250986e+10  00106       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_immigration_month.limit(5).toPandas()) #nfp - (nfp = Not for Production Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2935901"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration_month.count() #nfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read Temperature data\n",
    "df_temperature = spark.read.format('csv').option('header', 'true').load('../../data2/GlobalLandTemperaturesByCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.7369999999999999</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt AverageTemperature AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01              6.068            1.7369999999999999  Århus   \n",
       "1  1743-12-01               None                          None  Århus   \n",
       "2  1744-01-01               None                          None  Århus   \n",
       "3  1744-02-01               None                          None  Århus   \n",
       "4  1744-03-01               None                          None  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_temperature.limit(5).toPandas()) #nfp\n",
    "df_temperature.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read convertion table for the port codes\n",
    "df_port_code_names = spark.read.format('csv').option('header', 'true').option('sep', ';').load('port_codes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>port_code</th>\n",
       "      <th>port_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  port_code                 port_name\n",
       "0       ALC                     ALCAN\n",
       "1       ANC                 ANCHORAGE\n",
       "2       BAR  BAKER AAF - BAKER ISLAND\n",
       "3       DAC             DALTONS CACHE\n",
       "4       PIZ    DEW STATION PT LAY DEW"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_port_code_names.limit(5).toPandas()) #nfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "We will explore data quality issues column-wise systematically, according to these aspects:\n",
    "- Usefulness for the proposed task (columns, rows)\n",
    "- Data type (categorical, int, float, text, id, string, ...)\n",
    "- Missing values\n",
    "- Duplicate data\n",
    "- (Optional) for numerical variables:\n",
    "    - Noisiness and type of noise (outliers, rounding errors, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1 Explore and clean Immigration Data\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " cicid    | 95441.0        \n",
      " i94yr    | 2016.0         \n",
      " i94mon   | 4.0            \n",
      " i94cit   | 528.0          \n",
      " i94res   | 129.0          \n",
      " i94port  | CHI            \n",
      " arrdate  | 20545.0        \n",
      " i94mode  | 1.0            \n",
      " i94addr  | NY             \n",
      " depdate  | 20554.0        \n",
      " i94bir   | 30.0           \n",
      " i94visa  | 2.0            \n",
      " count    | 1.0            \n",
      " dtadfile | 20160401       \n",
      " visapost | MDD            \n",
      " occup    | null           \n",
      " entdepa  | G              \n",
      " entdepd  | O              \n",
      " entdepu  | null           \n",
      " matflag  | M              \n",
      " biryear  | 1986.0         \n",
      " dtaddto  | 09302016       \n",
      " gender   | F              \n",
      " insnum   | null           \n",
      " airline  | IB             \n",
      " admnum   | 9.250657813E10 \n",
      " fltno    | 06253          \n",
      " visatype | B2             \n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring Immigration Data - nfp\n",
    "df_immigration_month.show(1, truncate=False, vertical=True)\n",
    "df_immigration_month.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Exploring distinct values in selected columns (NfP)\n",
    "df_immigration_month.filter(col('count') == 1.0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- There are some columns which we won't need for the scope of this project, so we will just drop them from the dataframe:\n",
    "    - 'count' adds no further value, since all entries are 1\n",
    "    - 'dtadfile' which is described as 'Date added to I-94 Files' is also not relevant for our scope\n",
    "    - 'i94bir' this information is redundant, since also contained within 'biryear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop three columns\n",
    "df_immigration_month = df_immigration_month.drop('count', 'dtadfile', 'i94bir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Most numeric columns where infered as `double` type, but are for practical purposes, better modelled as `integer` type. We will cast them to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert relevant numeric type columns to IntegerType()\n",
    "int_type_cols = ['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94addr', 'i94res', 'i94mode', 'i94visa', 'biryear']\n",
    "df_imm_clean_1 = df_immigration_month\n",
    "\n",
    "for i in int_type_cols:\n",
    "    df_imm_clean_1 = df_imm_clean_1.withColumn(i, df_imm_clean_1[i].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Now we will convert all three date columns `arrdate`, `depdate` ,`dtaddto` to a proper string date format: 'yyyy-MM-DD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# arrdate and depdate to dateformat and yyyy-MM-dd\n",
    "df_imm_clean_2 = df_imm_clean_1\\\n",
    ".withColumn('arrdate', udf_to_datetime_sas('arrdate'))\\\n",
    ".withColumn('depdate', udf_to_datetime_sas('depdate'))\n",
    "\n",
    "# dtaddto restring to yyyy-MM-dd\n",
    "df_imm_clean_2 = df_imm_clean_2.withColumn('dtaddto', to_date(col('dtaddto'), 'MMddyyyy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Let's now count for missing values in every column. Results written here pre-ante:\n",
    "    - For all columns: there are no `NA` or `NaN` values\n",
    "        - to check, you can run `df_cleaning_1.select([count(when(isnan(c), c)).alias(c) for c in df_cleaning_1.columns]).show()`\n",
    "    - The distribution of `null` values for the columns' dataframes is as follows\n",
    "        - ![null-dist-values-imm-data](pics/data-immigration_null-value-dist.PNG)\n",
    "        - With these results, we can safely delete the columns `i94addr, visapost, occup, insnum, matflag`, since the percentage of missing values is too large and/or are not that relevant to our project. Additionally, the column `entdepu` has more than 90% missing values and since this column, as well as the related `entdepa` and `entdepd` are not much relevant for the purpose of this project, we will also delete all three.\n",
    "        - For the remaining columns with missing values, we will leave as is: since the percentage is not too high and none of the columns with a higher percentage of missing values are of numerical type to use a fill strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Nfp\n",
    "df_imm_clean_2.select([count(when(isnull(c), c)).alias(c) for c in df_imm_clean_2.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm_clean_3 = df_imm_clean_2.drop('i94addr', 'visapost', 'occup', 'insnum', 'entdepu', 'entdepa', 'entdepd', 'matflag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm_clean_3.show(2) #nfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Since for this project we are only interested in tourist travels and as we saw, the column `i94visa` has no missing values, we can filter out the `Business (=1)` and `Student (=3)` entries in the `i94visa` column, and ultimately drop the column (redundant)\n",
    "- Furthermore since the information contained in `i94yr` and `i94mon` is already contained within the column `arrdate`, we will also drop this columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_imm_clean_4 = df_imm_clean_3\\\n",
    ".filter(col('i94visa') == 2)\\\n",
    ".drop('i94visa', 'i94yr', 'i94mon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+-------+----------+-------+----------+-------+----------+------+-------+--------------+-----+--------+\n",
      "|cicid|i94cit|i94res|i94port|   arrdate|i94mode|   depdate|biryear|   dtaddto|gender|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+-------+----------+-------+----------+-------+----------+------+-------+--------------+-----+--------+\n",
      "|95441|   528|   129|    CHI|2016-04-01|      1|2016-04-10|   1986|2016-09-30|     F|     IB|9.250657813E10|06253|      B2|\n",
      "|95442|   528|   129|    CHI|2016-04-01|      1|2016-04-02|   1986|2016-09-30|     M|     AA|9.248179983E10|00069|      B2|\n",
      "|95444|   528|   528|    KAN|2016-04-01|      1|2016-05-11|   1964|2016-09-30|     F|     NK|9.242971203E10|00826|      B2|\n",
      "|95445|   528|   528|    CIN|2016-04-01|      1|2016-06-22|   1958|2016-09-30|     F|     AM|9.242279203E10|00630|      B2|\n",
      "+-----+------+------+-------+----------+-------+----------+-------+----------+------+-------+--------------+-----+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm_clean_4.show(4) # nfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- We will deal with duplicate data further down the line, when we create the analytics tables according to the data model in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2935901\n",
      "2501880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2501880"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nfp\n",
    "print(df_imm_clean_3.count())\n",
    "print(df_imm_clean_4.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2 Explore and clean Temperature Data\n",
    "Document steps necessary to clean the temperature data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- A quick glance at the temperature data and to the description of the data, reveals two things (source: https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data):\n",
    "    1. It contains data for many cities outside of the United States\n",
    "        - here we will filter the rows, so that only those for the US remain\n",
    "    2. It contains data ranging from the 18th century up until and including 2013\n",
    "        - here we will use only the last five years available. We will use this as a basis to build an average of the temperature data, since there is no data fot 2016. We use five years instead of just one, to build a more robust average, since tourism is quite dependent on weather conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# nfp\n",
    "df_temperature.show(2)\n",
    "df_temperature.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter for temperature data in the United States & drop the `Country` column\n",
    "df_temp_0 = df_temperature\\\n",
    ".filter(col('Country') == 'United States')\\\n",
    ".drop('Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert date column `dt` from string to date, for easier filtering and correct data representation type\n",
    "df_temp_1 = df_temp_0\\\n",
    ".withColumn('dt', to_date(col('dt'), 'yyyy-MM-dd'))\\\n",
    ".filter(col('dt') >= datetime(2008, 9, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+----------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|      City|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+----------+--------+---------+\n",
      "|2008-09-01|20.453000000000007|                         0.17|Alexandria|  39.38N|   76.99W|\n",
      "|2008-09-01|            21.199|                        0.254|   Anaheim|  32.95N|  117.77W|\n",
      "|2008-09-01|            18.676|                        0.212| Allentown|  40.99N|   74.56W|\n",
      "+----------+------------------+-----------------------------+----------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show results - nfp\n",
    "df_temp_1.orderBy(col('dt').asc()).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Both temperature `AverageTemperature`, `AverageTemperatureUncertainty` columns are of type string, even tough they contain numeric information. So we will convert this to numeric type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert temperature data to numeric\n",
    "df_temp_1 = df_temp_1\\\n",
    ".withColumn('AverageTemperature', col('AverageTemperature').cast(DecimalType()))\\\n",
    ".withColumn('AverageTemperatureUncertainty', col('AverageTemperatureUncertainty').cast(DecimalType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- AverageTemperature: decimal(10,0) (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: decimal(10,0) (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nfp - Check if dt and temperature column have the correct \n",
    "df_temp_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Now we will check for missing values (`null`) or for not valid values like`NaN`\n",
    "    - We will see that the results are consistent with the summary statistics of the source: https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "    - In this case there is just one missing value for the average temperature, which we will remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------+----+--------+---------+\n",
      "| dt|AverageTemperature|AverageTemperatureUncertainty|City|Latitude|Longitude|\n",
      "+---+------------------+-----------------------------+----+--------+---------+\n",
      "|  0|                 1|                            1|   0|       0|        0|\n",
      "+---+------------------+-----------------------------+----+--------+---------+\n",
      "\n",
      "+------------------+-----------------------------+----+--------+---------+\n",
      "|AverageTemperature|AverageTemperatureUncertainty|City|Latitude|Longitude|\n",
      "+------------------+-----------------------------+----+--------+---------+\n",
      "|                 0|                            0|   0|       0|        0|\n",
      "+------------------+-----------------------------+----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Nfp\n",
    "df_temp_1.select([count(when(isnull(c), c)).alias(c) for c in df_temp_1.columns]).show()\n",
    "df_temp_1.drop('dt').select([count(when(isnan(c), c)).alias(c) for c in df_temp_1.drop('dt').columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_2 = df_temp_1.filter(~isnull(col('AverageTemperature')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Next, we will check for duplicate data. As we'll see, there are no duplicates across all columns\n",
    "- We will also check for conflicting Temperature / Latitude / Longitude data for the same city on the same date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove duplicate data\n",
    "df_temp_3 = df_temp_2.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15127"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nfp - Checking for conflicting data on a given date for a given city\n",
    "df_temp_3\\\n",
    ".groupBy(col('dt'), col('City')).count()\\\n",
    ".count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- As we can see, there are more than 500 date-city combinations with conflicting data either in the temperature columns or in the latitude / longitude ones\n",
    "- To fix this, we are going to group by date and city and aggregate the rest columns as follows:\n",
    "    - AverageTemperature: mean\n",
    "    - AverageTemperatureUncertainty: mean\n",
    "    - Latitude: first\n",
    "    - Longitude: first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_4 = df_temp_3\\\n",
    ".groupBy(col('dt'), col('City'))\\\n",
    ".agg(\n",
    "    mean('AverageTemperature').alias('avg_temperature'),\n",
    "    mean('AverageTemperatureUncertainty').alias('avg_temperature_uncertainty'),\n",
    "    first('Latitude').alias('latitude'),\n",
    "    first('Longitude').alias('longitude')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15127\n",
      "+----------+-----------+---------------+---------------------------+--------+---------+\n",
      "|        dt|       City|avg_temperature|avg_temperature_uncertainty|latitude|longitude|\n",
      "+----------+-----------+---------------+---------------------------+--------+---------+\n",
      "|2008-09-01|     Irving|        24.0000|                     0.0000|  32.95N|   96.70W|\n",
      "|2008-10-01|       Waco|        19.0000|                     0.0000|  31.35N|   98.01W|\n",
      "|2008-10-01|    Windsor|        10.0000|                     0.0000|  42.59N|   82.91W|\n",
      "|2008-12-01|West Jordan|        -1.0000|                     1.0000|  40.99N|  112.90W|\n",
      "+----------+-----------+---------------+---------------------------+--------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nfp\n",
    "df_temp_4.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Finally, we deal with some aspects of the numeric columns (`avg_temperature` and `avg_temperature_uncertainty`:\n",
    "    1. rounding: We thought about rounding both temperature columns, but since we are going to aggregate this data later, we are going to skip the rounding here\n",
    "    2. check if max and min of `avg_temperature` are within the max (39.7) and min (-42.7) values, as given in: https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "    3. check the max `avg_temperature_uncertainty` value and delete any rows, if any, with a uncertainty higher than 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# nfp - 2. Check the max and min values of avg_temperature - nfp\n",
    "print('Max:')\n",
    "print(df_temp_4.agg({\"avg_temperature\": \"max\"}).collect()[0]['max(avg_temperature)'])\n",
    "print('Min:')\n",
    "print(df_temp_4.agg({\"avg_temperature\": \"min\"}).collect()[0]['min(avg_temperature)'])\n",
    "\n",
    "# nfp - 3. Check avg_temperature_uncertainty over 5 degrees celsius - nfp\n",
    "df_temp_4.filter(col('avg_temperature_uncertainty') > 5.0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+---------------------------+--------+---------+\n",
      "|        dt|       City|avg_temperature|avg_temperature_uncertainty|latitude|longitude|\n",
      "+----------+-----------+---------------+---------------------------+--------+---------+\n",
      "|2008-09-01|     Irving|        24.0000|                     0.0000|  32.95N|   96.70W|\n",
      "|2008-10-01|       Waco|        19.0000|                     0.0000|  31.35N|   98.01W|\n",
      "|2008-10-01|    Windsor|        10.0000|                     0.0000|  42.59N|   82.91W|\n",
      "|2008-12-01|West Jordan|        -1.0000|                     1.0000|  40.99N|  112.90W|\n",
      "|2009-01-01|Saint Louis|        -4.0000|                     0.0000|  39.38N|   89.48W|\n",
      "+----------+-----------+---------------+---------------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nfp\n",
    "df_temp_4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- A final note on what we learned during the cleaning process: the 'count' for the cleaned table is around 15000 rows. If you count the number of unique cities in the set for example with `df_temp_4.select(col('City')).distinct()` it comes to 248. Here it becomes clear, that the temperature average is not daily but monthly for every city. A quick look at the primary source of the data confirms this thought: http://berkeleyearth.org/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Since the purpose of this data project is to prepare granular analytics tables with tourist arrivals and departures to and from the cities in the United States and their relation to the temperature, the two first obvious choices for tables are:\n",
    "    - Dimension table 1: `port_temperature` > containing the temperature data of the ports, where tourists arrive to the US\n",
    "    - Fact table: `tourist_entries` > containing the admissions to the US for the year 2016\n",
    "    - We decided on a third table `travelers` as well, to separate the facts (admissions to the us) from the information related to the personas entering the US. In this way, further information about the travelers could be easily added in the future to this dimensional table, instead of putting even more information in the already large fact table. This could be done, e.g. using the attribute of `admission_id`.\n",
    "\n",
    "- Note: in the following data model, the data types are just for guidance, but do not match the exact type used here in PySpark. The data model can be summarized as follows:\n",
    "    - ![chosen-data-model](pics/data-model-capstone.png)\n",
    "    \n",
    "- Note: We also  thought to include a separete time table but without a millisecond-timestamp and the fact table being based on days, it does not make much sense. Furthermore, the goal of the analytics tables is to provide a base for aggregations and further high-levels analysis, for which a time table would be overdone.\n",
    "\n",
    "- As already mentioned, the purpose of this data model is to prepare analytics tables with clean and robust information on which analysis and aggregations on travelers data from Mexico and the Northern Triangle can be performed. With this background and for four concrete reasons, we chose a relational data model to model our data:\n",
    "    1. The relational model is by nature intuitive and easy to understand and thus facilitates the usage of the data for analytics purposes; specially for people that do not fully want to or need to understand the raw data\n",
    "    2. We don't know a-priori the specific queries that are going to be used on the dataset for it to be justified to use an NoSQL / Non-Relational approach\n",
    "    3. Even though the fact table is quite large, PySpark and the possibilities of distributed systems can cope with these and even larger amounts of data so in the case that someones wants to add more temperature, travelers or arrivals data\n",
    "    4. The relational data model allows for clear and strong data-types for each columns, which helps with data integrity. And even tough, PySpark does not strictly enforce constrains such as primary key, it has a strong columnar data-integrity and strong built-in functions to build help build such constrains on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "##### 3.2.1 Creating the table `port_temperature`\n",
    "- The basis for this analytics table is the cleaned temperature data from 2009 to 2013 for US cities To create the new table:\n",
    "\n",
    "1. we will average the temperature data over the five years for each city and each month of the year. To access this information we will create a column `month_of_year` and drop the original date column, since the year information would be misleading.\n",
    "2. for `latitude` and `longitude` we will aggregate by `first`.\n",
    "3. the primary key for this table will be a new column called `port_id`, which we will create by matching the `City` name with the dictionary data for the column `i94port` from the immigration data.\n",
    "4. as the final step of this pipeline, we will remove any duplicates\n",
    "\n",
    "##### 3.2.2 Creating the table `tourist_entries`\n",
    "- The basis for the `tourist_entries`table will be the cleaned and prepared immigration data. We decided **not** to aggregate the facts over cities, dates or any other attribute - since this provides the maximal granularity over the data and thus the most aggregation-flexibility for future analytics use cases. So the steps included for creating this table are:\n",
    "1. selecting the relevant columns and renaming them, as to match the data model (see diagram above)\n",
    "2. removing any duplicates (during the cleaning process we had already established that the `cicid` was unique across all rows)\n",
    "\n",
    "##### 3.2.3 Creating the table `travelers`\n",
    "- The `travelers` table we'll create from the clean immigration data. The steps are similar as for the `tourist_entries` table. The connection between this table and the fact table will be the `admission_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2501880"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nfp\n",
    "print(df_imm_clean_4.count())\n",
    "df_imm_clean_4.select('cicid').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data & Data Quality Checks\n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15127"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nfp\n",
    "df_temp_4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 3.2.1 Creating 'port_temperature'\n",
    "# Create the month_of_year and day_of_month column\n",
    "df_temp_5 = df_temp_4\\\n",
    ".withColumn('month_of_year', date_format(col(\"dt\"), \"MM\"))\\\n",
    "\n",
    "# Group by the combination of day_of_month and month_of_year\n",
    "df_temp_6 = df_temp_5\\\n",
    ".groupBy(col('month_of_year'), col('City'))\\\n",
    ".agg(\n",
    "    mean('avg_temperature').alias('avg_temperature'),\n",
    "    mean('avg_temperature_uncertainty').alias('avg_temperature_uncertainty'),\n",
    "    first('latitude').alias('latitude'),\n",
    "    first('longitude').alias('longitude')\n",
    ")\n",
    "\n",
    "# Add a unique id as a primary key for the table 'port_temperature'\n",
    "df_temp_final = df_temp_6\\\n",
    ".select(monotonically_increasing_id().alias('city_temp_id'),\n",
    "        col('month_of_year'),\n",
    "        col('City').alias('city'),\n",
    "        col('avg_temperature'),\n",
    "        col('avg_temperature_uncertainty'),\n",
    "        col('latitude'),\n",
    "        col('longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temp_final.write.mode('overwrite').partitionBy('month_of_year').parquet('output/port_temperature/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 3.2.2 Creating 'tourist_entries'\n",
    "df_imm_prep_1 = df_imm_clean_4\\\n",
    ".withColumn('arrival_month', date_format(col('arrdate'), 'dd'))\n",
    "\n",
    "df_imm_prep_2 = df_imm_prep_1\\\n",
    ".join(df_port_code_names, on=(df_imm_prep_1.i94port == df_port_code_names.port_code) , how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tourist_entries = df_imm_prep_2.alias('imm')\\\n",
    ".join(df_temp_final.alias('temp'), expr(\"imm.port_name == upper(temp.city) AND imm.arrival_month == temp.month_of_year\"), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tourist_entries = tourist_entries\\\n",
    ".select(col('cicid').alias('cicid'),\n",
    "    col('arrdate').alias('arrival_date'),\n",
    "    col('depdate').alias('departure_date'),\n",
    "    col('i94port').alias('port_code'),\n",
    "    col('port_name'),\n",
    "    col('city_temp_id'),\n",
    "    col('i94mode').alias('entry_mode'),\n",
    "    col('dtaddto').alias('date_added'),\n",
    "    col('airline'),\n",
    "    col('fltno').alias('flight_number'),\n",
    "    col('admnum').alias('admission_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>port_code</th>\n",
       "      <th>port_name</th>\n",
       "      <th>city_temp_id</th>\n",
       "      <th>entry_mode</th>\n",
       "      <th>date_added</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>admission_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1238203</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>YGF</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-09-18</td>\n",
       "      <td>YX</td>\n",
       "      <td>04400</td>\n",
       "      <td>9.116002e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1287403</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>2016-04-12</td>\n",
       "      <td>5T6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-06</td>\n",
       "      <td>*GA</td>\n",
       "      <td>N4709</td>\n",
       "      <td>9.295341e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1294872</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>NYL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-06</td>\n",
       "      <td>*GA</td>\n",
       "      <td>XARAV</td>\n",
       "      <td>9.299068e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1295918</td>\n",
       "      <td>2016-04-07</td>\n",
       "      <td>2016-04-08</td>\n",
       "      <td>YGF</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-06</td>\n",
       "      <td>YX</td>\n",
       "      <td>04400</td>\n",
       "      <td>9.297364e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1376197</td>\n",
       "      <td>2016-05-07</td>\n",
       "      <td>2016-07-08</td>\n",
       "      <td>NYL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-11-05</td>\n",
       "      <td>365</td>\n",
       "      <td>S0506</td>\n",
       "      <td>9.559488e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cicid arrival_date departure_date port_code port_name city_temp_id  \\\n",
       "0  1238203   2016-04-07     2016-04-09       YGF      None         None   \n",
       "1  1287403   2016-04-07     2016-04-12       5T6      None         None   \n",
       "2  1294872   2016-04-07     2016-04-13       NYL      None         None   \n",
       "3  1295918   2016-04-07     2016-04-08       YGF      None         None   \n",
       "4  1376197   2016-05-07     2016-07-08       NYL      None         None   \n",
       "\n",
       "   entry_mode  date_added airline flight_number  admission_id  \n",
       "0           1  2016-09-18      YX         04400  9.116002e+10  \n",
       "1           1  2016-10-06     *GA         N4709  9.295341e+10  \n",
       "2           1  2016-10-06     *GA         XARAV  9.299068e+10  \n",
       "3           1  2016-10-06      YX         04400  9.297364e+10  \n",
       "4           1  2016-11-05     365         S0506  9.559488e+10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nfp\n",
    "display(tourist_entries.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>port_code</th>\n",
       "      <th>port_name</th>\n",
       "      <th>city_temp_id</th>\n",
       "      <th>entry_mode</th>\n",
       "      <th>date_added</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>admission_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>560211</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>AA</td>\n",
       "      <td>00069</td>\n",
       "      <td>9.266813e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>560424</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>AV</td>\n",
       "      <td>00408</td>\n",
       "      <td>9.265914e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>560514</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04-20</td>\n",
       "      <td>CM</td>\n",
       "      <td>235</td>\n",
       "      <td>6.731406e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>561003</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>AA</td>\n",
       "      <td>00091</td>\n",
       "      <td>9.268941e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>561004</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-17</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>AA</td>\n",
       "      <td>00091</td>\n",
       "      <td>9.268947e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>561005</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-17</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>AA</td>\n",
       "      <td>00091</td>\n",
       "      <td>9.268945e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>561338</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>TK</td>\n",
       "      <td>00009</td>\n",
       "      <td>9.268937e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>561862</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-14</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>AM</td>\n",
       "      <td>00404</td>\n",
       "      <td>9.269116e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>561992</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>None</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>Y4</td>\n",
       "      <td>940</td>\n",
       "      <td>6.743497e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>562054</td>\n",
       "      <td>2016-04-03</td>\n",
       "      <td>2016-04-10</td>\n",
       "      <td>CHI</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>1254130450434</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>UA</td>\n",
       "      <td>05587</td>\n",
       "      <td>9.267102e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cicid arrival_date departure_date port_code port_name   city_temp_id  \\\n",
       "0  560211   2016-04-03     2016-04-04       CHI   CHICAGO  1254130450434   \n",
       "1  560424   2016-04-03     2016-04-06       CHI   CHICAGO  1254130450434   \n",
       "2  560514   2016-04-03     2016-04-21       CHI   CHICAGO  1254130450434   \n",
       "3  561003   2016-04-03     2016-04-05       CHI   CHICAGO  1254130450434   \n",
       "4  561004   2016-04-03     2016-04-17       CHI   CHICAGO  1254130450434   \n",
       "5  561005   2016-04-03     2016-04-17       CHI   CHICAGO  1254130450434   \n",
       "6  561338   2016-04-03     2016-04-04       CHI   CHICAGO  1254130450434   \n",
       "7  561862   2016-04-03     2016-04-14       CHI   CHICAGO  1254130450434   \n",
       "8  561992   2016-04-03           None       CHI   CHICAGO  1254130450434   \n",
       "9  562054   2016-04-03     2016-04-10       CHI   CHICAGO  1254130450434   \n",
       "\n",
       "   entry_mode  date_added airline flight_number  admission_id  \n",
       "0           1  2016-10-02      AA         00069  9.266813e+10  \n",
       "1           1  2016-10-02      AV         00408  9.265914e+10  \n",
       "2           1  2016-04-20      CM           235  6.731406e+08  \n",
       "3           1  2016-10-02      AA         00091  9.268941e+10  \n",
       "4           1  2016-10-02      AA         00091  9.268947e+10  \n",
       "5           1  2016-10-02      AA         00091  9.268945e+10  \n",
       "6           1  2016-10-02      TK         00009  9.268937e+10  \n",
       "7           1  2016-10-03      AM         00404  9.269116e+10  \n",
       "8           1  2016-10-02      Y4           940  6.743497e+08  \n",
       "9           1  2016-10-03      UA         05587  9.267102e+10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nfp\n",
    "display(tourist_entries.filter(col('port_name') == 'CHICAGO').limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# nfp\n",
    "tourist_entries.select(col('cicid')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tourist_entries.write.mode('overwrite').parquet('output/tourist_entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 3.2.3 Creating 'travelers'\n",
    "travelers = df_imm_clean_4 \\\n",
    ".select(col('admnum').alias('admission_id'),\n",
    "    col('i94cit').alias('citizen_of'),\n",
    "    col('i94res').alias('resident_of'),\n",
    "    col('biryear').alias('year_of_birth'),\n",
    "    col('gender'),\n",
    "    col('visatype').alias('visa_type')) \\\n",
    ".distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "travelers.write.mode('overwrite').partitionBy('year_of_birth').parquet('output/travelers/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- We will perform three data quality checks, two integrity constraints on the unique keys and one count check over all tables:\n",
    "    1. Quality check 1 (QC1): will consist of checking for missing values in the primary key of each table. For this we, select just the column containig the primary key, filter for existing null-values and then take a count. If the value is any other than zero, we print a warning. Since PySpark nor Parquet (or chosen output data-format) do not enforce this type of integrity constraints - as opposed to a Postgres database - it makes sense to check them after running the data pipelines. The same applies for the next check.\n",
    "    2. Quality check 2 (QC2): will check if the primary key column of each of the three tables contains unique values. To do this, we will compare the count of the full table against the distinct values of the primary key column. If they do not match, we print a warning.\n",
    "    3. Quality check 3 (QC3): in this count check, we make sure that non of the tables are empty after the data pipelines have ran. If this is the case, we also print a warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### QC1: Run check for missing values in the primary keys of all three tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: The data check ran successfully > The primary key in the port_temperature table does not contain missing values\n"
     ]
    }
   ],
   "source": [
    "# Quality check 1: Checking for missing values in the primary key\n",
    "temp_id_missing_value_count = df_temp_final.select('city_temp_id').filter(isnull(col('city_temp_id'))).count()\n",
    "\n",
    "if temp_id_missing_value_count != 0:\n",
    "    print('Warning: The primary key *city_temp_id* in the port_temperature table contains missing values')\n",
    "else:\n",
    "    print('Success: The data check ran successfully > The primary key in the port_temperature table does not contain missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quality check 1: Checking for missing values in the primary key\n",
    "entries_id_missing_value_count = tourist_entries.select(col('cicid')).filter(isnull(col('cicid'))).count()\n",
    "\n",
    "if entries_id_missing_value_count != 0:\n",
    "    print('Warning: The primary key *cicid* in the tourist_entries table contains missing values')\n",
    "else:\n",
    "    print('Success: The data check ran successfully > The primary key in the tourist_entries table does not contain missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quality check 1: Checking for missing values in the primary key\n",
    "travelers_id_missing_value_count = travelers.select(col('admission_id')).filter(isnull(col('admission_id'))).count()\n",
    "\n",
    "if travelers_id_missing_value_count != 0:\n",
    "    print('Warning: The primary key *admission_id* in the travelers table contains missing values')\n",
    "else:\n",
    "    print('Success: The data check ran successfully > The primary key in the travelers table does not contain missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### QC2: Run check for the unique-constraint on the primary keys of all three tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: The primary key in the travelers table contains only unique values\n"
     ]
    }
   ],
   "source": [
    "# Quality check 2: Check for unique values in the primary key\n",
    "row_count_full = df_temp_final.count()\n",
    "row_count_primary_key = df_temp_final.select(col('city_temp_id')).count()\n",
    "\n",
    "if row_count_full != row_count_primary_key:\n",
    "    print('Warning: The primary key in the port_temperature table contains duplicate values')\n",
    "else:\n",
    "    print('Success: The primary key in the port_temperature table contains only unique values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: The primary key in the tourist_entries table contains only unique values\n"
     ]
    }
   ],
   "source": [
    "# Quality check 2: Check for unique values in the primary key\n",
    "row_count_full = tourist_entries.count()\n",
    "row_count_primary_key = tourist_entries.select(col('cicid')).count()\n",
    "\n",
    "if row_count_full != row_count_primary_key:\n",
    "    print('Warning: The primary key in the tourist_entries table contains duplicate values')\n",
    "else:\n",
    "    print('Success: The primary key in the tourist_entries table contains only unique values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Quality check 2: Check for unique values in the primary key\n",
    "row_count_full = travelers.count()\n",
    "row_count_primary_key = travelers.select(col('admission_id')).count()\n",
    "\n",
    "if row_count_full != row_count_primary_key:\n",
    "    print('Warning: The primary key in the travelers table contains duplicate values')\n",
    "else:\n",
    "    print('Success: The primary key in the travelers table contains only unique values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### QC3: Run check for non empty tables\n",
    "- Check that the tables indeed are integral and have some rows and are not fully empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: The port_temperature table is not empty\n"
     ]
    }
   ],
   "source": [
    "# Quality check 3: check for existing rows in the table\n",
    "table_row_count = df_temp_final.count()\n",
    "\n",
    "if table_row_count == 0:\n",
    "    print('Warning: The port_temperature table appears to be empty')\n",
    "else:\n",
    "    print('Success: The port_temperature table is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Quality check 3: check for existing rows in the table\n",
    "table_row_count = tourist_entries.count()\n",
    "\n",
    "if table_row_count == 0:\n",
    "    print('Warning: The tourist_entries table appears to be empty')\n",
    "else:\n",
    "    print('Success: The tourist_entries table is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: The travelers table is not empty\n"
     ]
    }
   ],
   "source": [
    "# Quality check 3: check for existing rows in the table\n",
    "table_row_count = travelers.count()\n",
    "\n",
    "if table_row_count == 0:\n",
    "    print('Warning: The travelers table appears to be empty')\n",
    "else:\n",
    "    print('Success: The travelers table is not empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "- A data dictionary for our data model has been provided in a tabular way in the file 'Data-Dictionary-Capstone.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up / Document the Project\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    - High Data Volume: In this project we deal with relatively large quantities of data. The whole immigration data contains almost three million rows and the temperature data more than eight million. The immigration data is only for 2016, but more recent data would probably be even larger ([Source](https://www.trade.gov/i-94-arrivals-program)). Spark was build to handle large quantities of data, and if a local Spark installation will not suffice, cloud distributed systems can be used to scale data processing.\n",
    "    - Non-transactionality: The type of data and goal of this project are not transactional in the sense, that there is no new data coming day-to-day, in which case a data warehouse approach might also be adequate. In this project, we are dealing with historic data, which we can clean and pre-process once and store as a 'Data Lake' to be used when needed\n",
    "    - Different data-types: for the first version of this project, we are dealing with only two different input data types: CSV and SAS. However, if the analysis of the tourism arrivals in the U.S. should be extended to include other information, this might come in the way of semi-structured or unstructured data. In this case, PySpark and common storage services the approach are more than able to deal with multiple data-types.\n",
    "    - Flexibility of Data Model: we proposed a conceptual data model, which we thought would be easy to understand and would serve for the goal of this project. However, the specific requirements could change or evolve, specially requirements for facts, dimension tables or for the aggregation level. If the data model ought to be changed or new tables to be added, PySpark provides a more flexibel and intuitive approach (Schema-on-Read), than to create a database everytime, create the tables and write the correct queries to insert the data.\n",
    "    - Certainly also an argument to have in mind: the cost of storage is usually significantly lower, in comparison to the higher cost of staging and processing large amounts of data on a cloud Data Warehouse service (e.g. Redshift).\n",
    "    \n",
    "\n",
    "- Propose how often the data should be updated and why.\n",
    "    - As mentioned above, the nature of this project is not transactional, but more to prepare historic - rather non-changing - data for analytics purposes. If the 'customer' is satisfied with the time range from the datasets used here (immigration data & travelers: year 2016, temperature data: average from 2009 - 2013), then the data does not have to be updated until the requirement changes.\n",
    "    - If the requierement changes, and immigration data from year 2020 or if more accurate and recent temperature becomes available, the ETL pipeline ought to be adjusted and run successfully. But if the requirements are still for historic and analytical purposes, then there would also be no need to update the data - until the requirements change.\n",
    "    - This applies for all three analytical tables (`tourist_entries`, `travelers`, `port_temperature`\n",
    "\n",
    "\n",
    "* Challenges / Critical observations / Ideas for further improvement:\n",
    "    - A port of arrival for a traveler is not always the end destination. So assuming the port of arrival to be the tourist destination is misleading. The information of the final destination in the initial column `i94addr` of the immigration data was missing for the majority of the data. However, this additional information could be of great use for the four countries (Mexico + the Northern Triangle) to analyze where their residents and citizens are going\n",
    "    - Temperature Data was from the years 2009 until 2013. This is another improvement possibility, to provide more current temperature data\n",
    "\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     - As mentioned above, Spark was build to handle huge quantities of data. A 100x increase in data would certainly be a challenge to a local Spark installation in one computer. However, this is where Spark really shines, that is, when it is used in distributed systems to split the load of data processing. This could be done for example in a scalable cluster on the cloud like AWS EMR\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     - This is one classical use case for using a 'workflow manager', where one can precisely schedule data engineering pipelines, tasks, run quality checks on them and monitor their result. Common tools are Apache Airflow, which works well with python, or Luigi which works well with Spark.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "     - Since our approach was a 'Data Lake' one, where the final data is saved into parquet files into storage - we would need a cloud storage service that can:\n",
    "         - be accesed from people in different places\n",
    "         - manage multiple concurrent connections\n",
    "         - give read access rights to at least 100+ people\n",
    "     - AWS S3 is an example of such a storage service, that with proper configuration, could fulfill these requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
